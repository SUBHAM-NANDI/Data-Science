{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4220c4-de73-4a38-9ac0-c7bf34ed55ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? \n",
    "# Provide an example to illustrate its application.\n",
    "\n",
    "# Q1. Min-Max Scaling: Min-Max scaling is a data preprocessing technique used to scale \n",
    "# numerical features to a specific range, typically between 0 and 1. It involves \n",
    "# transforming the data in such a way that the minimum value of the feature becomes 0,\n",
    "# the maximum value becomes 1, and all other values are scaled proportionally in between.\n",
    "# Min-Max scaling is particularly useful when the features have different ranges, \n",
    "# and it helps to bring all features to a common scale.\n",
    "\n",
    "# The formula for Min-Max scaling is as follows:\n",
    "# Scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "# Example: Let's consider a dataset with a feature \"age\" representing the age of \n",
    "# individuals. The minimum age is 20, and the maximum age is 60. We want to scale\n",
    "# these ages to a range between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "# Original ages: [20, 30, 40, 50, 60]\n",
    "\n",
    "# Min-Max scaled ages:\n",
    "# Scaled_age = (age - 20) / (60 - 20)\n",
    "# Scaled_ages = [0.0, 0.25, 0.5, 0.75, 1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2336232a-758f-4988-b560-d48b1bbad659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does \n",
    "# it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "# The Unit Vector technique, also known as Unit Length scaling or Normalization, is a feature\n",
    "# scaling method that scales each data point in a feature vector to have a \n",
    "# Euclidean norm (length) of 1. It is often used in machine learning algorithms that rely on\n",
    "# distance metrics, such as k-nearest neighbors, to avoid undue influence from features \n",
    "# with larger scales.\n",
    "\n",
    "# The formula for Unit Vector scaling is as follows:\n",
    "# Scaled_value = original_value / ||feature_vector||\n",
    "\n",
    "# Where ||feature_vector|| is the Euclidean norm (length) of the feature vector.\n",
    "\n",
    "# Difference from Min-Max Scaling:\n",
    "# The main difference between Unit Vector scaling and Min-Max scaling is the scaling range.\n",
    "# Unit Vector scaling scales features to have a length of 1, and the values can vary between\n",
    "# -1 and 1 (in case of negative values). Min-Max scaling scales features to a specific \n",
    "# user-defined range, usually between 0 and 1.\n",
    "\n",
    "# Example: Let's consider a dataset with a feature \"height\" representing \n",
    "# the height of individuals. The original heights are as follows: [160, 175, 150, 180, 165]\n",
    "\n",
    "# Unit Vector scaled heights:\n",
    "# Scaled_height = height / ||heights||, where ||heights|| = sqrt(160^2 + 175^2 + 150^2 + 180^2 + 165^2) = 356.71\n",
    "# Scaled_heights = [0.449, 0.491, 0.420, 0.504, 0.463]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b75f6c8-ce34-4b46-a142-22517a944326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in \n",
    "# dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "# PCA is a dimensionality reduction technique used to transform high-dimensional\n",
    "# data into a lower-dimensional space while preserving its most important patterns\n",
    "# and variations. It achieves this by identifying orthogonal principal components  \n",
    "#that capture the most significant variance in the data. The first principal component\n",
    "# explains the largest variance, the second principal component explains the second-largest\n",
    "# variance, and so on. By retaining a subset of these principal components, PCA reduces \n",
    "# the number of features while retaining the most critical information.\n",
    "\n",
    "# Application of PCA in Dimensionality Reduction:\n",
    "# PCA is commonly used in various fields, such as image recognition, data compression,\n",
    "# and feature extraction, where high-dimensional data can be difficult to work with \n",
    "# and may lead to overfitting in machine learning models.\n",
    "\n",
    "# Example:\n",
    "# Consider a dataset with three features: \"age,\" \"income,\" and \"spending habits.\" \n",
    "# Performing PCA on this dataset may reveal that the first principal component is primarily \n",
    "# driven by income, the second principal component by age, and the third by spending habits. \n",
    "# We can choose to keep only the first two principal components to reduce \n",
    "# the dimensionality of the data from three features to two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4b2bea-8f83-41a2-847e-0bd5bcebeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, \n",
    "# and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "# PCA is a specific method for feature extraction. It identifies new features \n",
    "# (principal components) that are linear combinations of the original features and represent \n",
    "# the most significant variations in the data. These principal components are ordered in terms\n",
    "# of their explained variance, allowing us to select a subset of them to represent the data \n",
    "# in a lower-dimensional space effectively.\n",
    "\n",
    "# Using PCA for Feature Extraction:\n",
    "# To use PCA for feature extraction, we follow these steps:\n",
    "\n",
    "# Standardize the data: Ensure that all features have zero mean and unit variance.\n",
    "# Calculate the covariance matrix: Compute the covariance matrix from the standardized data.\n",
    "# Compute the eigenvectors and eigenvalues: The eigenvectors represent the principal components, \n",
    "# and the eigenvalues represent the variance explained by each principal component.\n",
    "# Sort eigenvectors: Sort the eigenvectors based on their corresponding eigenvalues \n",
    "# in descending order to identify the most important principal components.\n",
    "# Choose the number of principal components: Select the desired number of principal\n",
    "# components to retain, reducing the dimensionality.\n",
    "# Project data onto the new feature space: Multiply the original data by the selected \n",
    "# eigenvectors to obtain the transformed data in the reduced feature space.\n",
    "# Example:\n",
    "# Consider a dataset with four features: \"height,\" \"weight,\" \"age,\" and \"income.\"\n",
    "# We perform PCA to extract the most important features, and let's assume that the first \n",
    "# two principal components account for 90% of the variance. We decide to retain only these \n",
    "# two components and discard the other two, effectively reducing the dataset's dimensionality\n",
    "# from four to two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf6d5ae-5a5f-44da-9cb9-d6a1a804b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service.\n",
    "# The dataset contains features such as price, rating, and delivery time.\n",
    "# Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "# In the context of a food delivery recommendation system, Min-Max scaling can be used to \n",
    "# preprocess the numerical features such as \"price,\" \"rating,\" and \"delivery time.\" \n",
    "# The goal is to bring these features to a common scale (usually between 0 and 1) to avoid \n",
    "# one feature dominating the others and to ensure that all features contribute equally\n",
    "# to the recommendation process.\n",
    "\n",
    "# Steps to use Min-Max Scaling for data preprocessing:\n",
    "\n",
    "#  Identify the numerical features: Select the features that need scaling, \n",
    "# which in this case are \"price,\" \"rating,\" and \"delivery time.\"\n",
    "# Find the minimum and maximum values for each feature: Calculate the minimum and maximum\n",
    "# values of each feature in the dataset.\n",
    "# Apply Min-Max scaling: For each data point in the features \"price,\" \"rating,\" and \"delivery time,\" \n",
    "# apply the Min-Max scaling formula to transform the values to the desired range (e.g., 0 to 1).\n",
    "# For instance, let's assume the original values for the features are as follows:\n",
    "\n",
    "# Price: [10, 15, 20, 25]\n",
    "# Rating: [3.5, 4.2, 4.8, 3.9]\n",
    "# Delivery time (minutes): [30, 45, 20, 60]\n",
    "# After Min-Max scaling, the values could look like this:\n",
    "\n",
    "# Scaled Price: [0.0, 0.33, 0.67, 1.0]\n",
    "# Scaled Rating: [0.0, 0.56, 1.0, 0.78]\n",
    "# Scaled Delivery time: [0.33, 0.67, 0.0, 1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50672684-af88-4c31-ad6d-8fa10404f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices.\n",
    "# The dataset contains many features, such as company financial data and market trends. \n",
    "# Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "# When working on a stock price prediction project with a dataset containing many \n",
    "# features like company financial data and market trends, PCA can be used to reduce \n",
    "# the dimensionality of the dataset. The high dimensionality of the data can lead to \n",
    "# challenges such as increased computational complexity, overfitting, and difficulty \n",
    "# in interpreting the results. PCA helps to address these issues by identifying \n",
    "# the most important patterns and reducing the number of features while preserving \n",
    "# the essential information.\n",
    "\n",
    "# Steps to use PCA for dimensionality reduction in stock price prediction:\n",
    "\n",
    "# Data Preprocessing: Ensure the dataset is preprocessed, including handling missing values, \n",
    "# scaling numerical features, and encoding categorical variables if any.\n",
    "\n",
    "# Standardization: Standardize the numerical features (e.g., financial data and market trends)\n",
    "# so that they have zero mean and unit variance. Standardization is crucial before applying PCA, \n",
    "# as it ensures that all features contribute equally to the principal components.\n",
    "\n",
    "# Apply PCA: Perform PCA on the standardized dataset to identify the principal components. \n",
    "# The number of principal components to be retained depends on the desired level of \n",
    "# dimensionality reduction and the amount of variance explained.\n",
    "\n",
    "# Determine the number of principal components: One can decide how many principal components \n",
    "# to retain based on the explained variance. For example, if the first few principal components \n",
    "# explain a significant portion of the variance (e.g., 95%), then retaining those components \n",
    "# will capture the most critical information while reducing the dimensionality.\n",
    "\n",
    "# Project data onto reduced feature space: Use the retained principal components to project \n",
    "# the original data onto the reduced feature space. The transformed data will have a reduced \n",
    "# number of features, which can be used as input for training a stock price prediction model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ecda8d1-d89d-4ccc-a2bb-4f06d15a18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform \n",
    "# Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "# To perform Min-Max scaling on the given dataset with values [1, 5, 10, 15, 20] and \n",
    "# transform them to a range of -1 to 1, follow these steps:\n",
    "\n",
    "# Find the minimum and maximum values in the dataset: Minimum = 1, Maximum = 20\n",
    "# Apply the Min-Max scaling formula:\n",
    "# Scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "# Applying the formula to each value:\n",
    "# Scaled_Values = [(1 - 1) / (20 - 1), (5 - 1) / (20 - 1), (10 - 1) / (20 - 1), \n",
    "#                  (15 - 1) / (20 - 1), (20 - 1) / (20 - 1)]\n",
    "\n",
    "# Simplified:\n",
    "# Scaled_Values = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Now, to transform these values to a range of -1 to 1:\n",
    "# Scaled_Values_Ranged = [(2 * x) - 1 for x in Scaled_Values]\n",
    "\n",
    "# The final Min-Max scaled values in the range of -1 to 1 are:\n",
    "# Scaled_Values_Ranged = [-1.0, -0.5, 0.0, 0.5, 1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411601d-99a2-4acf-86d4-990184f9d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "# Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "# In the dataset containing features [height, weight, age, gender, blood pressure],\n",
    "# we can use PCA for feature extraction to reduce the dimensionality while retaining \n",
    "# the most significant information.\n",
    "\n",
    "# Steps for Feature Extraction using PCA:\n",
    "\n",
    "# Standardization: Standardize the numerical features (height, weight, age, and blood pressure) \n",
    "# to have zero mean and unit variance. This step is essential to ensure that all features \n",
    "# contribute equally to the principal components.\n",
    "\n",
    "# Apply PCA: Perform PCA on the standardized dataset to calculate the principal components \n",
    "# and their corresponding eigenvalues.\n",
    "\n",
    "# Select the number of principal components: Decide on the number of principal components to retain \n",
    "# based on the explained variance or a predetermined threshold. The explained variance indicates \n",
    "# how much information each principal component captures.\n",
    "\n",
    "# Project data onto reduced feature space: Project the original data onto the reduced feature space\n",
    "# defined by the selected principal components. This transformation will produce a new dataset \n",
    "# with reduced dimensions.\n",
    "\n",
    "# The number of principal components to retain depends on the desired level of dimensionality \n",
    "# reduction and the trade-off between reducing complexity and preserving information. \n",
    "# A common approach is to choose the number of principal components that explain a significant\n",
    "# portion of the variance, for example, retaining enough components to capture 95% or 99% of the variance.\n",
    "\n",
    "# In practice, you can analyze the cumulative explained variance ratio of the principal components \n",
    "# and decide on the number of components that satisfy your requirements. For example, \n",
    "# if the first three principal components explain 90% of the variance, you may choose to retain\n",
    "# those three components for feature extraction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
